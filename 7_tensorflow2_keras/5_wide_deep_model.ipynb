{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide Deep Model\n",
    "Can be used for Classification and Regression (Google use it for Google App as Recommendation Algorethmus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 sparse feature vector\n",
    "Sparse representation is used when feature vectors are expected to have a large percentage of zeros in them, as opposed to dense vectors. <br>\n",
    "**For Example:** <br>\n",
    "Subject = {computer_science, culture, math, etc} <br>\n",
    "**One-Hot:** <br>\n",
    "Subject = [ 1, 0, 0, 0 ] one represents computer_science<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Feature Multiplication\n",
    "You can combine such feature vector with other information and represent them together as a matrix <br>\n",
    "\n",
    "**+** very efficient way <br>\n",
    "\n",
    "**-** you need to design it manually <br>\n",
    "\n",
    "**-** overfitting, every feature will multiplicate with other features, this is a kind of \"memory for all of information\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Dense feature vector\n",
    "For the same example like Subject = {computer_science, culture, math, etc} you can also represent the information in following way (dense featrue vector), which will show you the distance between vectors:\n",
    "<br>\n",
    "computer_science = [ 0.3, 0.25, 0.1, 0.4 ] \n",
    "<br>\n",
    "culture = [ 0.5, 0.2, 0.2, 0.1 ]\n",
    "<br>\n",
    "math = [ 0.33, 0.35, 0.1, 0.2 ]\n",
    "<br>\n",
    "etc = [ 0.4, 0.15, 0.7, 0.4 ]\n",
    "<br> <br>\n",
    "**3.1 Word2Vector**\n",
    "<br>\n",
    "use exactly this way this calculate the similarity of words. As result we got: \n",
    "<br>\n",
    "man - women = king - queen\n",
    "<br><br>\n",
    "**+** it will also take the meaning of such things into consideration <br>\n",
    "**+** compatible also with the information, which didn't appeared in training phase <br>\n",
    "**+** less manually work <br>\n",
    "**-** underfitting for example it recommend you something, what you don't really want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Wide Deep Model\n",
    "![title](imag/wide_deep_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Use wide deep model to predict california housing price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.callbacks import History\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Loading datasets\n",
    "housing = fetch_california_housing()\n",
    "# Split data into tran, test and validation\n",
    "# by default train_test_split split data in 3: 1 -> train and test -> default param -> test_size = 0.25\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(housing.data, housing.target, random_state = 7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, random_state = 11)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "# Data normalization\n",
    "# before normalization\n",
    "print(np.max(x_train), np.min(x_train))\n",
    "\n",
    "# perform normalization\n",
    "scaler = StandardScaler()\n",
    "# 1. data in x_train is int32, we need to convert them to float32 first \n",
    "# 2. convert x_train data from \n",
    "#    [None, 28, 28] -> [None, 784] \n",
    "#       -> after all reshape back to [None, 28, 28]\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled  = scaler.transform(x_test)\n",
    "\n",
    "# after normalization\n",
    "# print(np.max(x_train_scaled), np.min(x_train_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in wide deep model we usualy use multiple inputs. for a simple example I just\n",
    "# made from one dataset two subsets:\n",
    "# the first one take first 6 columns and the second the last 6\n",
    "input_wide = keras.layers.Input(shape=[5]) # wide part\n",
    "\n",
    "input_deep = keras.layers.Input(shape=[6]) # beginn to build deep part\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_deep)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "\n",
    "# concate two parts together\n",
    "concat = keras.layers.concatenate([input_wide, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "# the same as multi-input, it is also possible to have a multi-output net\n",
    "'''\n",
    "output2 = keras.layers.Dense(1)(hidden2)\n",
    "model = keras.models.Model(inputs = [input_wide, input_deep],\n",
    "                           outputs = [output, output2])\n",
    "'''\n",
    "model = keras.models.Model(inputs = [input_wide, input_deep],\n",
    "                           outputs = output)\n",
    "model.summary()\n",
    "# mean_squared_error make model as regression\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = \"sgd\", metrics = [\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience = 5, min_delta = 1e-2)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate input data as wide and deep\n",
    "\n",
    "x_train_scaled_wide = x_train_scaled[:, :5]\n",
    "x_train_scaled_deep = x_train_scaled[:, 2:]\n",
    "x_test_scaled_wide = x_test_scaled[:, :5]\n",
    "x_test_scaled_deep = x_test_scaled[:, 2:]\n",
    "x_valid_scaled_wide = x_valid_scaled[:, :5]\n",
    "x_valid_scaled_deep = x_valid_scaled[:, 2:]\n",
    "\n",
    "history = model.fit([x_train_scaled_wide, x_train_scaled_deep], \n",
    "                    y_train, \n",
    "                    validation_data=([x_valid_scaled_wide, x_valid_scaled_deep], y_valid),\n",
    "                    epochs = 100, \n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history: History):\n",
    "    pd.DataFrame(history.history).plot(figsize = (8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate([x_test_scaled_wide, x_test_scaled_deep], y_test)\n",
    "# one_hot encoded results\n",
    "predictions = model.predict([x_test_scaled_wide, x_test_scaled_deep])\n",
    "index = 40\n",
    "\n",
    "for indx in range(index):\n",
    "    print(y_test[indx], predictions[indx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
